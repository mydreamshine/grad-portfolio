default:
    trainer: ppo
    batch_size: 1024
    beta: 5.0e-3
    buffer_size: 10240
    epsilon: 0.2
    hidden_units: 128
    lambd: 0.95
    learning_rate: 3.0e-4
    learning_rate_schedule: linear
    max_steps: 5.0e4
    memory_size: 256
    normalize: false
    num_epoch: 3
    num_layers: 2
    time_horizon: 64
    sequence_length: 64
    summary_freq: 1000
    use_recurrent: false
    vis_encode_type: simple
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99


Ball Brain: # Ball Brain에 명시하지 않은 설정값들은 default 섹션에서 자동으로 추가된다.
    normalize: true # 입력되는 벡터 관측값이 정규화되어 있는가
    gamma: 0.90 # 미래의 보상을 기준으로 현재에 평가할 때 쓰는 할인율. 수치가 높을 수록 Agent가 즉각적인 보상보단 미래의 보상을 좀 더 추구하게 됨.
    lamdda: 0.99 # Agent의 학습 성향. 수치가 높을 수록 학습의 다양성이 높아진다. 반대로 수치가 낮을 수록 학습의 성향이 편향적이 된다.
    buffer_size: 4096 # 모델이 갱신되는 step 주기
    batch_size: 256 # gradient descent가 갱신되는 step 주기
    max_steps: 5.0e4 # 5.0e4: 50000 학습 종료 step

FoodCollectorLearning:
    normalize: false
    beta: 5.0e-3
    batch_size: 1024
    buffer_size: 10240
    max_steps: 1.0e5

BouncerLearning:
    normalize: true
    max_steps: 1.0e6
    num_layers: 2
    hidden_units: 64

PushBlockLearning:
    max_steps: 5.0e4
    batch_size: 128
    buffer_size: 2048
    beta: 1.0e-2
    hidden_units: 256
    summary_freq: 2000
    time_horizon: 64
    num_layers: 2

SmallWallJumpLearning:
    max_steps: 1.0e6
    batch_size: 128
    buffer_size: 2048
    beta: 5.0e-3
    hidden_units: 256
    summary_freq: 2000
    time_horizon: 128
    num_layers: 2
    normalize: false

BigWallJumpLearning:
    max_steps: 1.0e6
    batch_size: 128
    buffer_size: 2048
    beta: 5.0e-3
    hidden_units: 256
    summary_freq: 2000
    time_horizon: 128
    num_layers: 2
    normalize: false

StrikerLearning:
    max_steps: 5.0e5
    learning_rate: 1e-3
    batch_size: 128
    num_epoch: 3
    buffer_size: 2000
    beta: 1.0e-2
    hidden_units: 256
    summary_freq: 2000
    time_horizon: 128
    num_layers: 2
    normalize: false

GoalieLearning:
    max_steps: 5.0e5
    learning_rate: 1e-3
    batch_size: 320
    num_epoch: 3
    buffer_size: 2000
    beta: 1.0e-2
    hidden_units: 256
    summary_freq: 2000
    time_horizon: 128
    num_layers: 2
    normalize: false

PyramidsLearning:
    summary_freq: 2000
    time_horizon: 128
    batch_size: 128
    buffer_size: 2048
    hidden_units: 512
    num_layers: 2
    beta: 1.0e-2
    max_steps: 5.0e5
    num_epoch: 3
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99
        curiosity:
            strength: 0.02
            gamma: 0.99
            encoding_size: 256

VisualPyramidsLearning:
    time_horizon: 128
    batch_size: 64
    buffer_size: 2024
    hidden_units: 256
    num_layers: 1
    beta: 1.0e-2
    max_steps: 5.0e5
    num_epoch: 3
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99
        curiosity:
            strength: 0.01
            gamma: 0.99
            encoding_size: 256

3DBallLearning:
    normalize: true
    batch_size: 64
    buffer_size: 12000
    summary_freq: 1000
    time_horizon: 1000
    lambd: 0.99
    beta: 0.001

3DBallHardLearning:
    normalize: true
    batch_size: 1200
    buffer_size: 12000
    summary_freq: 1000
    time_horizon: 1000
    max_steps: 5.0e5
    beta: 0.001
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.995

TennisLearning:
    normalize: true
    max_steps: 2e5

CrawlerStaticLearning:
    normalize: true
    num_epoch: 3
    time_horizon: 1000
    batch_size: 2024
    buffer_size: 20240
    max_steps: 1e6
    summary_freq: 3000
    num_layers: 3
    hidden_units: 512
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.995

CrawlerDynamicLearning:
    normalize: true
    num_epoch: 3
    time_horizon: 1000
    batch_size: 2024
    buffer_size: 20240
    max_steps: 1e6
    summary_freq: 3000
    num_layers: 3
    hidden_units: 512
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.995

WalkerLearning:
    normalize: true
    num_epoch: 3
    time_horizon: 1000
    batch_size: 2048
    buffer_size: 20480
    max_steps: 2e6
    summary_freq: 3000
    num_layers: 3
    hidden_units: 512
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.995

ReacherLearning:
    normalize: true
    num_epoch: 3
    time_horizon: 1000
    batch_size: 2024
    buffer_size: 20240
    max_steps: 1e6
    summary_freq: 3000
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.995

HallwayLearning:
    use_recurrent: true
    sequence_length: 64
    num_layers: 2
    hidden_units: 128
    memory_size: 256
    beta: 1.0e-2
    num_epoch: 3
    buffer_size: 1024
    batch_size: 128
    max_steps: 5.0e5
    summary_freq: 1000
    time_horizon: 64

VisualHallwayLearning:
    use_recurrent: true
    sequence_length: 64
    num_layers: 1
    hidden_units: 128
    memory_size: 256
    beta: 1.0e-2
    num_epoch: 3
    buffer_size: 1024
    batch_size: 64
    max_steps: 5.0e5
    summary_freq: 1000
    time_horizon: 64

VisualPushBlockLearning:
    use_recurrent: true
    sequence_length: 32
    num_layers: 1
    hidden_units: 128
    memory_size: 256
    beta: 1.0e-2
    num_epoch: 3
    buffer_size: 1024
    batch_size: 64
    max_steps: 5.0e5
    summary_freq: 1000
    time_horizon: 64

GridWorldLearning:
    batch_size: 32
    normalize: false
    num_layers: 1
    hidden_units: 256
    beta: 5.0e-3
    buffer_size: 256
    max_steps: 5.0e5
    summary_freq: 2000
    time_horizon: 5
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.9

BasicLearning:
    batch_size: 32
    normalize: false
    num_layers: 1
    hidden_units: 20
    beta: 5.0e-3
    buffer_size: 256
    max_steps: 5.0e5
    summary_freq: 2000
    time_horizon: 3
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.9
